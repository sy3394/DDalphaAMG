\documentclass[a4paper,12pt]{scrartcl}
\usepackage{ucs}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{mathptmx}
\usepackage[scaled=.92]{helvet}
\usepackage{courier}
\usepackage{graphicx}
\usepackage[pdftex]{color}
\usepackage[pdftex]{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsmath}
\usepackage{amssymb}

\begin{document}

\section{The DDalphaAMG Solver Library}

\subsection{Introduction}
This MPI code in \texttt{C}-language solves the following degenerate twisted-mass Wilson-Dirac equations:
\begin{equation*}
D_{\mathrm{TM}}(\mu_\mathrm{TM}) \psi = \eta
\end{equation*}
using an aggregation-based algebraic multigrid (AMG) method, where 
\begin{equation*}
D_{\mathrm{TM}}(\mu_\mathrm{TM}) = D_{W}+i\mu_\mathrm{TM}\Gamma_5
\end{equation*}
and $D_W$ is a (clover-improved) Wilson Dirac matrix.  In the matrix, $i\mu_\mathrm{TM}\Gamma_5$ corresponds to a twisted-mass term with the twisted mass, $\mu_\mathrm{TM}$.  

%The clover-improved Wilson Dirac operator: 
%\begin{equation*}
%D_{\mathrm{cW}} = D_{\rm W} - \frac{c_{sw}}{32}\sum_{\mu,\nu = 0}^3 \left(\gamma_\mu\gamma_\nu\right)\otimes \left(Q_{\mu\nu}(x) - Q_{\nu\mu}(x)\right)
%\end{equation*}
%where 
%$Q_{\mu\nu}(x) = \sum\limits_{n=0}^{3}U^{\circlearrowleft^n}_{\mu\nu}(x)$, $U_{\mu\nu}^\circlearrowleft(x) =U_\nu(x+\hat{\mu})U_\mu^\dagger(x+\hat{\nu})U_\nu^\dagger(x)U_\mu(x)$, c.f., $U_{\mu\nu}(x) =U_\mu(x)U_\nu(x+\hat{\mu})U_\mu^\dagger(x+\hat{\nu})U_\nu^\dagger(x)$
%\begin{align*}
%U_{\mu\nu}(x) =& U_\mu(x)U_\nu(x+\hat{\mu})U_\mu^\dagger(x+\hat{\nu})U_\nu^\dagger(x) \\
%U_{\mu\nu}^\circlearrowleft(x) =& U_\nu(x+\hat{\mu})U_\mu^\dagger(x+\hat{\nu})U_\nu^\dagger(x)U_\mu(x)
%%Q_{\mu\nu}(x) =& U_{\mu\nu}(x) + U_{\mu\nu}^\circlearrowleft(x-\hat{\mu}) + U_{\mu\nu}^{\circlearrowleft^2}(x-\hat{\mu}-\hat{\nu})+U_{\mu\nu}^{\circlearrowleft^3}(x-\hat{\nu})
%\end{align*}

If a \texttt{C}-flag, \texttt{-DHAVE\_TM1p1}, is switched on (cf. Section~\ref{dirac_op:ss},~\ref{additionalcflags}), the code solves the equation with the degenerated twisted mass operator: 
\begin{equation*}
D_{ND}(\mu_\mathrm{TM}) = (D_{W}\otimes I_2) + i\mu_\mathrm{TM}(\Gamma_5\otimes \tau_3) - \varepsilon(I\otimes\tau_1)
=
\begin{pmatrix}
D_{\mathrm{TM}}(\mu_\mathrm{TM}) & -\varepsilon\\
-\varepsilon & D_{\mathrm{TM}}(-\mu_\mathrm{TM})
\end{pmatrix}
\end{equation*}
where $\tau$ is a Pauli matrix in the flavor-space and $\varepsilon$ a flavor-mixing parameter.  The solutions and rhs then contain both $u$ and $d$ flavors, i.e., 
\begin{equation*}
\Psi = \begin{pmatrix}
\psi_u\\
\psi_d
\end{pmatrix},\,
H = \begin{pmatrix}
\eta_u\\
\eta_d
\end{pmatrix}
\end{equation*}
%\begin{align*}
%D_{ND}(&\mu) = (D_{W}\otimes I_2) + i\mu_\mathrm{TM}(\Gamma_5\otimes \tau_3) \\
%&\Rightarrow
%D_{ND}(\mu) \Psi  = 
%\begin{pmatrix}
%D_{\mathrm{TM}}(\mu_\mathrm{TM}) & 0\\
%0 & D_{\mathrm{TM}}(-\mu_\mathrm{TM})
%\end{pmatrix}
%\begin{pmatrix}
%\psi_u\\
%\psi_d
%\end{pmatrix} = 
%\begin{pmatrix}
%D_{W}\psi_u\\
% D_{W}\psi_d
%\end{pmatrix}
%+
%\begin{pmatrix}
%+i\mu\Gamma_5\psi_u\\
%-i\mu\Gamma_5\psi_d
%\end{pmatrix}
%\end{align*}

Here, $\eta$ and $\psi$ can be multiple vectors.  Inversion of $D_\mathrm{TM}$ or $D_{ND}$ for all right-hand sides (rhs) in $\eta$ is done simultaneously via auto-vectorization.  Because of this feature, the user first needs to decide on the number of vectors processed together in a vectorized manner, which is called \texttt{BASE\_LOOP\_COUNT} in Makefile and set its value at compile time.  As all \texttt{for}-loops in this code, involving rhs and test vectors for construction of interpolation operators, loops over \texttt{for}-loops of the basic loop count, their numbers should then be a multiple of this basic loop count.  

Herein, the smoother was chosen as the Schwarz alternating procedure (SAP).  The Dirac operator depends on a configuration $U$ which therefore is required as input.  This along with other parameters such as the mass parameter $m_0 = 1/(2\kappa)-4$ can be adjusted and specified in a parameter file, supplied to the program as an input file.  An example of such a file is provided as \texttt{sample.ini} in the main directory.  A complete list of parameters with their meanings can be found in \texttt{sample\_devel.ini}.

We expect that the user of this code is familiar with the required basics (usage of \texttt{C}-compilers, MPI libraries as well as lattice QCD, configurations and so on).  This code also supports OpenMP-threading.  To enable OpenMP, the user needs to define a flag \texttt{-DOPENMP} in Makefile and specify the number of OpenMP threads in the parameter file, in addition to exporting the environment variable \texttt{OMP\_NUM\_THREADS}.  For understanding the method behind this \texttt{C}-code or even for a brief introduction into the structure of the Wilson Dirac operator from lattice QCD, we refer the interested reader to~\cite{Frommer:2013kla,FroKaKrLeRo13,RottmannPhD}. Implementation details can be found in~\cite{RottmannPhD}.

\subsection{Compilation}\label{compile_wilson:ss}
The main directory contains an example for Makefile. This makefile needs a few adjustments (compiler and MPI library) before compiling the code on your machine. Please note, that the code was built in a generic precision. Thus, the actual code for compilation is created by the makefile via pattern substitution right before the compilation. This part of the makefile should not be removed.
  
Once the necessary adjustments of the makefile are done, the command
\begin{center}
\texttt{make} \texttt{-f} \textit{yourmakefile} \texttt{-j} \texttt{execs}
\end{center}
should compile the whole entire Wilson solver code, such that it is ready to be run. The makefile contains additional rules "library" and "documentation" for compiling the code as a library and for compiling the user documentation as well as "exec-tests" for compiling a test routines and "install" to make an install directory specified via setting PREFIX variable.  The library interface can be found in the "include" folder.  Additional \texttt{CFLAGS} are described in Section~\ref{additionalcflags}. 

\subsection{Running the Code}\label{run_wilson:ss}
Once the code has been compiled, the executables \texttt{DD\_alpha\_AMG} and \texttt{DD\_alpha\_AMG\_devel} can be found in the \texttt{bin} directory and run with typical mpirun commands. The latter executable runs a (slower) debug version with additional check routines. The executables accept a user-specified parameter file as an optional command-line argument (default:\texttt{sample.ini}) so that the code can be run with, e.g.,
\begin{center}
\texttt{mpirun -np} \textit{number\_of\_ranks} \texttt{bin/DD\_alpha\_AMG} \textit{yourinputfile}
\end{center}
The path corresponding to your input file can be stated either relatively to the main directory or as an absolute path. For easy usage, we recommend to use a run-script. An example is already included in main directory and can be easily modified for your personal needs.

To use this code as a solver library, the user needs to link the static libraries, \texttt{libDDalphaAMG.a} and \texttt{libDDalphaAMG\_devel.a}, in the \texttt{lib} directory in a usual manner.  Explanation of the interface functions along with parameters is provided in \texttt{src/DDalphaAMG.h}.  By following the header file, the user should be ready to deploy the interface functions in one's own code.

In order to run the code successfully, you will need a \textit{configuration} and a \textit{parameter file}. In the upcoming sections, you will be guided through the most important aspects of the parameter file in order to make them consistent with the code and run the code with your configuration and parameters.

\subsection{Adjusting Parameters}\label{param_wilson:ss}
In this section, we take a closer look at the parameter files \texttt{sample.ini} and \texttt{sample\_devel.ini}. The first is a shorter user-oriented version, the latter a longer one for developers. Parameters with (absolutely) obvious meaning are omitted here.

\subsubsection{Configurations}
First of all, the path to your configuration has to be specified (again relatively to the main directory or absolutely).  The desired data layout is illustrated in Appendix. In the folder \texttt{conf/convert}, one can find a converter which converts OPENQCD/DD-HMC configurations into the desired layout appropriate to our code.  Our code is also capable of reading configurations in the lime format.  To do so requires the user to set the format parameter as \texttt{format:} \texttt{1}.  To enable this feature, the user needs to link the proper lime library when compiling the code (cf. Section~\ref{additionalcflags}). 
Note that you can store the paths of different configurations in the parameter file by simply writing them in the same file.   The one  read and used by the code is always the first occurrence of path specification starting with ``\texttt{configuration:}''.  This behavior also holds for other parameters. 
%In case you want to use two different configurations, one for the hopping term and one for the clover term, you have to enter the respective paths with the prefixes  ``\texttt{hopp cnfg:}'' and ``\texttt{clov cnfg:}'' in your input file. In this case, the prefix ``\texttt{configuration:}'' should not appear in your input file.

\subsubsection{Geometry}
In the geometry part of the input file, the geometry of the lattice (in \texttt{depth0}) and the coarser lattices (in \texttt{depth1}, \texttt{depth2}, $\ldots$) as well as how the problem should be parallelized has to be defined.  Then, the code needs the following details with some restrictions on their values.  Let $\mathbb{N}$ be a set of natural numbers.
\begin{itemize}
\item \texttt{depth0}
  \begin{itemize}
    \item \texttt{d0 global lattice:~T Z Y X} determines the lattice dimensions of your configuration.
    \item \texttt{d0 local lattice:~T Z Y X} determines the lattice dimensions on a single processor for each direction, $\mu$.  We assume
    \begin{equation*}
    \mu_g(i)/\mu_l(i) \in \mathbb{N} 
    \end{equation*}
     where $\mu_{g/l}(i)$ is a global/local lattice dimension in the $\mu$-direction at the $i^{ th}$ level.
     For the number of MPI processes $np$, we have
       $$ np = \prod_{\mu=1}^4 \mu_g(0)/\mu_l(0) \, . $$
  \end{itemize}
\item \texttt{depth$i$} ($i \geq 0$)
  \begin{itemize}
    \item \texttt{d$i$ block lattice:~T Z Y X} determines the size of the Schwarz blocks. We assume 
\begin{equation*}
\mu_l(i)/\mu_b(i)\in \mathbb{N}
\end{equation*}    
     where $\mu_b(i)$ is a block dimension in the $\mu$ direction at the $i^{th}$ level.  Furthermore, we need at least two blocks per processor. If possible, we recommend a block size of $4^4$.
    \item Define the coarsening ratio, i.e, the aggregate size, to be $$agg_\mu(i)\equiv \mu_g(i)/\mu_g(i+1).$$  We assume $$agg_\mu(i) \in \mathbb{N}.$$  As $agg_\mu(i)$ determines the coarsening ratio, we also need
    \begin{equation*}
    \mu_l(i)/agg_\mu(i),\, agg_\mu(i)/ \mu_b(i) \in \mathbb{N}.
    \end{equation*}
    If possible, we recommend $agg_\mu(0)=4$ and $agg_\mu(i>0)=2$. 
    \item We also assume $\mu_g(i)/\mu_l(i)$ as a function of $i$ to be monotonically decreasing for all $\mu$. The code allows processes to idle on coarser grids. This can happen since we have less workload on coarser grids. The stated assumption means that the number of processors that are idle can not decrease as we go to a coarser lattice, and once a processor idles on a certain level, it will idle on all coarser levels.    
    \item \texttt{d$i$ preconditioner cycles}: number of preconditioner cycles in every preconditioner call on level~$i$.  This number is not used on the coarsest level.
    \item \texttt{d$i$ post smooth iter}: number of post smoothing iterations applied on level~$i$.  This number is not used on the coarsest level.
    \item \texttt{d$i$ block iter}: number of iterations for the block solver in SAP on level~$i$.  This number is not used on the coarsest level.
    \item \texttt{d$i$ test vectors}: number of test vectors used on level~$i$. We recommend using $20$ test vectors for $i=0$ and $30$ for $i>0$.  This number is not used on the coarsest level.
    \item \texttt{d$i$ setup iter}: number of setup iterations for AMG on the level~$i$.  This number is not used on the coarsest level.
    \item \texttt{mu factor}: a factor multiplying $\mu_\mathrm{TM}$ on the level~$i$.  We recommend to set it to 1 except at the bottom level.  A non-unit factor leads to a larger condition number and faster convergence at the bottom where precision of the solutions are not required.
  \end{itemize}
\end{itemize}
Further information about how to tune the method sufficiently can be found in~\cite{Frommer:2013kla,FroKaKrLeRo13,RottmannPhD,Alexandrou:2016izb}. When running the code for the first time, it is enough to adjust the global, local, and block lattice at \texttt{depth0} as well as $m_0$ and $c_{sw}$.  Lattice dimensions at coarse levels, when not given, are determined from those at the top level via
\begin{align*}
\mu_g(i) &= \mu_g(i-1)/\mu_b(i-1)\\
\mu_l(i) &= \mu_g(i-1)/\mu_l(i-1).
\end{align*}
with potential adjustment for $\mu_l(i)$.  The code tries to set $\mu_b(i>0) = 2,3$.  If this auto-determination is not possible, the program lets you know it and exits.  If interested, look into \texttt{readin.c}.

\subsubsection{Dirac Operator}\label{dirac_op:ss}
Adjust the parameters \texttt{m0} and \texttt{csw} according to your $m_0$ and $c_{sw}$ for which you want to solve the Dirac equation. In the header file \texttt{src/clifford.h}, you can adjust the Clifford algebra basis representation as well. The basis representations of BMW-c, OPENQCD/DD-HMC, and QCDSF are pre-implemented.  In case you want to implement your own representation, please be aware of our conventions for the Wilson-Dirac operator in~\cite{FroKaKrLeRo13,RottmannPhD}.

Twisted-mass parameters, $\mu_\mathrm{TM}$ and $\varepsilon$, should also be chosen according to your problem.  This code accepts a shift of the twisted mass parameter whose value can be different on even and odd sites by setting \texttt{mu even shift} and \texttt{mu odd shift} to different values.  When specifying \texttt{mu even shift}, the user needs to determine its value for each rhs.  They can be all the same, but even in the case, those values need to be written out.  When running an executable instead of a library, after solving for a selected flavor, the code will proceed to solve for the other flavor in a single flavor mode, if \texttt{addDownPropagator} is set to non-zero value.

In the two-flavor mode with \texttt{-DHAVE\_TM1p1} switched on, the flavor-mixing parameter, $\varepsilon$, becomes relevant.  Its value needs to be adjusted properly.  Actually, this can be 0, in which case $D_{ND}$ becomes degenerate.  With non-zero value for \texttt{force\_2flavours}, the code solves
\begin{equation*}
D_{ND}(\mu_\mathrm{TM}) \Psi
=
\begin{pmatrix}
D_{\mathrm{TM}}(\mu_\mathrm{TM}) & 0\\
0 & D_{\mathrm{TM}}(-\mu_\mathrm{TM})
\end{pmatrix}
\begin{pmatrix}
\psi_u\\
\psi_d
\end{pmatrix} = H,
\end{equation*}
i.e., inversion for $u$ and $d$ part is done simultaneously, which might speed up the computation.  With this choice, doublet interface solvers, \texttt{DDalphaAMG\_solve\_doublet*}, becomes available.  Otherwise, the code behaves as if it is in a single flavor mode when $\varepsilon=0$.

\subsubsection{Multilevel Parameters}
In the multilevel part, most of the parameters do not require any additional tuning. Some additional remarks are given here:
\begin{itemize}
  \item \texttt{odd even preconditioning}:  The coarsest grid can be chosen to be solved using odd-even preconditioned GMRES.  Likewise at the coarsest level, SAP can be applied via odd-even preconditioned minimal residual iteration. If you switch on this parameter, i.e., set it to any other value except \texttt{0}, you have to make sure that $\prod_\mu \mu_l(i) \geq 2$ and $\mu_g(i) \equiv 0\, (\text{mod. }2)$ for every $\mu$ where $c$ denotes the coarsest level.
  \item \texttt{mixed precision}:   The user should set this to \texttt{0} to use the whole method in double precision. The value \texttt{1} indicates that preconditioning is performed in single precision.  However, the outer top-level FGMRES method is still in double precision.  In addition to the value \texttt{0} and \texttt{1}, the value \texttt{2} signifies a mixed precision outer FGMRES routine, for which one the relative residual estimation in FGMRES might be less accurate.
  \item \texttt{kcycle}: set it to the value \texttt{1} to switch it on or \texttt{0} to switch it off. If \texttt{kcycle} is switched on, the standard multigrid V-cycle is replaced by K-cycle.  K-cycle is a cycling strategy, which can be explained as follows: In a two-level method with GMRES as a coarse grid solver, the coarse grid solver is replaced by an FGMRES method preconditioned with another two-level method of the same kind. It can be viewed as a W-cycle where each level is wrapped by FGMRES. This FGMRES wrapper can be adjusted with a restart length \texttt{kcycle length}, a number restart cycles \texttt{kcycle restarts} and a tolerance for the relative residual \texttt{kcycle tolerance}. Note that all tolerances in this code are concidered as relative and non-squared.
\end{itemize}

\subsubsection{Fast Accurate Block Linear krylOv Solver (Fabulous) Parameters}
At each level, the user can tune the fabulous-solver related parameters.  Not all the parameters are relevant to a given choice of the fabulous solver.  For detailed explanation of the solvers as well as the meaning of the parameters, the user should refer to the documentation of the library found at \url{https://gitlab.inria.fr/solverstack/fabulous}.  The parameters include:
\begin{itemize}
\item \texttt{di fabulous orthogonalization scheme}: determines the orthogonalization scheme.  The value 0 indicates Modified Gram-Schmidt, 1 for Iterated Modified Gram-Schmidt, 2 for  Classical Gram-Schmidt, and 3 for Iteratied Classical Gram-Schmidt.
\item \texttt{di fabulous orthogonalization type}: determines orthogonalization type.  The value 0 corresponds to RUHE variant (vector-by-vector) and 1 to BLOCK variant (block-by-block).
\item \texttt{di fabulous max kept dir}:  determines the maximum number of kept direction per iteration.  This parameter is meaningful only for the solver using inexact breakdown.   For other solvers, it is not taken into account. You can set this parameter to -1 if you want it to match the number of rhs.
\item \texttt{di number of deflating eigenvectors for fabulous}: determines the number of eignevectors used for solvers involving deflation.
\item \texttt{di max number of mat-vec product for fabulous}: determines the maximum number of matrix vector product to be done
\item \texttt{di fabulous compute real residual}: determines if residuals should be computed after each iteration.  This may slow down BGMRES a lot and used only for debugging.
\end{itemize}

\subsubsection{Tracking Parameters}
The code offers the possibility to track some parameters. In order to switch this feature on, set \texttt{evaluation} to any value except 0. If an update of setup or shift is required, set the respective parameter to any other value than \texttt{0}.

\subsubsection{Default Values}
Most of the parameters in the input file are pre-defined with default values which can be checked or even modified in \texttt{src/readin.c}. We provide an input file called \texttt{sample.ini} with a quite small number of parameters and another one called \texttt{sample\_devel.ini} with all parameters that can be used. You can extend the short version by any parameter from the long version.

\subsection{Additional CFLAGS for compilation} \label{additionalcflags}
There are additional \texttt{CFLAGS} in the makefile, mostly for debugging and I/O, that you can switch on/off:
\begin{itemize}
  \item \texttt{-DPARAMPOUTPUT}: prints a summary of the input parameters.
  \item \texttt{-DPROFILING}: prints a bunch of profiling information, useful for optimization.
  \item \texttt{-DTRACK\_RES}: prints relative residual norms during the solve. 
  \item \texttt{-DCOARSE\_RES}: prints all coarser GMRES final relative residuals during setup and solve.
  \item \texttt{-DFGMRES\_RESTEST}: computes the true residual after the FGMRES solve and prints it. This is particularly useful when using the parameter \texttt{mixed precision:~2} since estimated and true residual norms can differ.
  \item \texttt{-DSCHWARZ\_RES}: prints all SAP final relative residuals during setup and solve.
  \item \texttt{-DTESTVECTOR\_ANALYSIS}: computes the eigenvalue-ishness of all test vectors during the setup phase.
  \item \texttt{-DSINGLE\_ALLREDUCE\_ARNOLDI}: modifies the Arnoldi iteration such that just on allreduce per iteration is needed. Therefore the norm of the next iterate is computed from inner product results. Please note that this can cause numerical instabilities in ill-conditioned cases. For further information, see~e.g.~\cite{RottmannPhD} and references therein.
  \item \texttt{-DHAVE\_LIME}: enables the code to read configurations in lime format. It requires an installed version of the c-lime library by USQCD and the enviroment variable \texttt{LIMEDIR} to be set with the installation directory. The functions in the header file \texttt{lime\_io.h} can be used to manage the io required for reading and saving the vectors.
  \item \texttt{-DHAVE\_FABULOUS}: enables the code to use block Krylov solvers.  It requires instillation of an external library, known as fabulous (Fast Accurate Block Linear krylOv Solver).  This will allow the user to use a variety of block Krylov solvers provided by the library.  To select one, set \texttt{di:~solver} to non-default value of 0.  Which value corresponds to which solver can be found out in \texttt{sample\_devel.ini}.
  \item \texttt{-DHAVE\_TM1p1}: enables the code to solve a degenerate twisted-mass Wilson-Dirac equation.  As the code may need to solve both $u$ and $d$ part simultaneously, i.e., to work with $\Psi$ and $H$, instead of $\psi_f$ and $\eta_f$ separately, it will allocate twice as much memory as it does in the single-flavor mode.  So be aware of memory usage when selecting this option.
\end{itemize}

\section{Appendix}
In this section, we offer additional information which might be helpful.

\subsection{Configuration Layout}
The configuration layout for our AMG solver has the following structure:
%
\begin{algorithm}[H]
  \caption{read conf}\label{readconf}
  \begin{algorithmic}[1]
    \FOR{$t=1$ to $n_t$}
      \FOR{$z=1$ to $n_s$}
	 \FOR{$y=1$ to $n_s$}
	    \FOR{$x=1$ to $n_s$}
	      \FOR{$mu=1$ to $4$}
		\STATE read $U_{t,z,y,x}(mu)$
	      \ENDFOR
	    \ENDFOR  
	 \ENDFOR  
      \ENDFOR  
    \ENDFOR  
  \end{algorithmic}
\end{algorithm}
%
\noindent where $U_{t,z,y,x}(mu) $ has to be stored row major.
%
\begin{algorithm}[H]
  \caption{read $U_{t,z,y,x}(mu)$}\label{readU}
  \begin{algorithmic}[1]
    \FOR{$i=1$ to $3$}
      \FOR{$j=1$ to $3$}
	\STATE read real($(U_{t,z,y,x}(mu))_{i,j}$)
	\STATE read imag($(U_{t,z,y,x}(mu))_{i,j}$)
      \ENDFOR  
    \ENDFOR
  \end{algorithmic}
\end{algorithm}
%
\noindent Herein the matrices $U_{t,z,y,x}(mu)$ have to be stored with $18$ \texttt{double} values.

\bibliographystyle{plain}
\bibliography{user_doc}

\end{document}